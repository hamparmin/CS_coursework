{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Collocation Dictionary\n",
    "\n",
    "English being my second language, I regularly encounter situations in which I am unable to decide whether I am using an appropriate adjective or adverb, pre- or posposition. One of my favourite places to turn to is http://www.ozdic.com/, an online GUI for the Oxford Collocations Dictionary (OCD). Collocations are sequences of word that co-occur regularly. Learning collocations helps language students sound more authentic. The OCD has been compiled by hand since the late 1950s by language experts and have been updated regularly to reflect changes in linguistic trends.\n",
    "\n",
    "Working on my original project idea of buliding an n-gram based 'intrinsic plagiarism detector' (which I deemed too difficult a task to do, but will attempt during my summer break), I came upon the idea that n-grams would be ideal to build my own collocation dictionary. The idea is very simple:\n",
    "\n",
    "1. Given a corpus of text, construct frequency tables for a a modified verison of n-grams, called skip-grams (with a linguistically justified max-skip parameter).\n",
    "2. Use these frequency tables to create Markov-probability matrices.\n",
    "3. Given an input word, use Markov matrices to retrieve most likely pre- and post postions of a word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip Grams\n",
    "\n",
    "The first step is the construction of skip-grams. Skip Grams are just a slight modification of n-grams. Unlike n-grams, where we want to find $n$ contiguous words from a sequence of words, in skip-grams words do not need to be contiguous. For example the tri-grams for the text \"English is my second language\" tri-grams would be:\n",
    "\n",
    "1. \"English is my\"\n",
    "2. \"is my second\"\n",
    "3. \"my second language\"\n",
    "\n",
    "Where the skip-grams of length 3 are much more numerous:\n",
    "\n",
    "1. \"English is my\"\n",
    "2. \"English is second\"\n",
    "3. \"English is language\"\n",
    "4. \"is my second\"\n",
    "5. \"is my language\"\n",
    "6. Etc., there exists altogether $k \\choose n$ skip-grams, where $k$ is the length of the total sequence and $n$ the length of the skip-gram. In this case ${5 \\choose 3}=10$ skip-grams.\n",
    "\n",
    "Skip-grams are useful as they allow us to take a larger sample of the corpus that can ignore some of the \"useless\" words such as connectives and common pre or post positions (e.g. the, an, and, or, etc.).\n",
    "\n",
    "## A dynamic solution\n",
    "\n",
    "While the runtime complexity of finding all n-grams in a sequence of $N$ words is $O(N)$, as illustrated above, finding skip-grams will optimally take the runtime of the number of $n$-length skip-grams for a sequence of $k$ words, thus $O({k \\choose n})$. In order to achieve this complexity however, as opposed to the suboptimal brute force solution of $O(k^n)$ we must devise a dynamic solution.\n",
    "\n",
    "Also for practical purposes, we will want to limit the number of skips when constructing our skip-grams. Then the optimal substructure for skip-grams of length $n$ and maximum skip of $m$ can be defined as follows. Let $S[p][q]$ be a set of skip-grams of length $q$ (with max skip of $m$) starting at the position $p$ in the input sequence. In order to generate all skip grams of length $q+1$, ending at position k, we can define :\n",
    "\n",
    "$S[p][q+1] = S[p] + S[p+1][q] + S[p+2][q] + ... S[p+m+1][q]$.\n",
    "\n",
    "Which means that we first need to take the word at the $p^{th}$ position, and then add all the skip grams of length $q$ startting from $p+1, p+2 ...,p+m+1$. This allows us to define a recursive relation for the subproblem (see below) and memoize our intermediate solutions to reduce the algorithm's runtime complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This book is ï»¿\n",
      "The Project Gutenberg EBook of Ulysses, by James it is 1539145 words long.\n"
     ]
    }
   ],
   "source": [
    "'''Load corpus.'''\n",
    "import collections\n",
    "import re\n",
    "\n",
    "#read in file\n",
    "f=open('ulysses.txt',encoding=\"utf8\") # i used james joyce's ulysses\n",
    "txt=f.read()\n",
    "f.close()\n",
    "print ('This book is', txt[:50] , \"it is\",len(txt), \"words long.\")\n",
    "\n",
    "#split by words\n",
    "words = re.split('[^A-Za-z]+', txt.lower())\n",
    "words = list(filter(None, words)) # Remove empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dynamic Construction of skip-grams. \n",
    "\n",
    "'''\n",
    "def skip_grams_recursion(words, p, n, m, memoi):\n",
    "    if n == 1:\n",
    "        #base case, if skip-gram only len 1\n",
    "        memoi[(p, n)] = [words[p]]\n",
    "    else:\n",
    "        # subproblem S[p][q+1]=S[p] + S[p+1][q] + S[p+2][q] + ... S[p+m+1][q]\n",
    "        current_skipgram = []\n",
    "        for idx in range(1, m + 2):\n",
    "            if p + idx < len(words) and n > 1:\n",
    "                if (p + idx, n - 1) in memoi:\n",
    "                    current_skipgram += memoi[(p + idx, n - 1)]\n",
    "                else:\n",
    "                    #recursive call for next skip-gram segment\n",
    "                    current_skipgram += skip_grams_recursion(words, p + idx, n - 1, m, memoi)\n",
    "                    \n",
    "        new_skipgram = [words[p] + \" \" + x for x in current_skipgram]\n",
    "        memoi[(p, n)] = new_skipgram\n",
    "    return memoi[(p, n)]\n",
    "\n",
    "def skip_grams(words, n, m):\n",
    "    memoi = collections.defaultdict(list) #use defaultdict to prevent key-value errors\n",
    "    skiplist = []\n",
    "    for idx in range(len(words) - n + 1):\n",
    "        skiplist += skip_grams_recursion(words, idx, n, m, memoi)\n",
    "    return skiplist\n",
    "\n",
    "'''\n",
    "Construction of dictionary of skip-gram tuples and their frequency.\n",
    "'''\n",
    "def skip_to_freq(words,n,m):\n",
    "    #create a dictionary of frequencies\n",
    "    out=dict()\n",
    "    skips=generate_ngram(words,2)\n",
    "    \n",
    "    # Populate skip-gram frequency dictionary\n",
    "    for i in range(len(words)-(n-1)):\n",
    "        key = tuple(words[i:i+n])\n",
    "        if key in out:\n",
    "            out[key] += 1\n",
    "        else:\n",
    "            out[key] = 1\n",
    "    return out\n",
    "'''\n",
    "Construct a Markov transition matrix from frequencies.\n",
    "'''\n",
    "\n",
    "def freq_to_markov(words,n=2,m=0):\n",
    "    #dict of all words transformed to a list\n",
    "    sdict=skip_to_freq(words,1,0)\n",
    "    sdict=list(sdict.keys())\n",
    "    \n",
    "    #dict of bigram frequencies\n",
    "    bfreq=skip_to_freq(words,2,1)\n",
    "    \n",
    "    #dimensions of markov matrix\n",
    "    dim=len(sdict)\n",
    "    \n",
    "    #initialise markov matrix\n",
    "    M = [[0 for x in range(dim)] for y in range(dim)]\n",
    "    \n",
    "    #fill out matrix\n",
    "    for i in range(dim):\n",
    "        for j in range(dim):\n",
    "            M[i][j]=bfreq.get(sdict[i]+sdict[j])\n",
    "    \n",
    "    return M\n",
    "\n",
    "M=freq_to_markov(corpus) #construct Markov matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Find the collocation of a given string using the skip-gram Markov matrices.\n",
    "\n",
    "'''\n",
    "\n",
    "def collocations(M,string,corpus,n=2,m=2):    \n",
    "    #dict of corpus words\n",
    "    sdict=skip_to_freq(words,1,0)\n",
    "    sdict=list(sdict.keys())\n",
    "\n",
    "    #check if word in corpus\n",
    "    if (string,) not in sdict:\n",
    "        raise ValueError('Word is not in corpus. Try another one!')\n",
    "    \n",
    "    #find index of element\n",
    "    idx=sdict.index((string,))\n",
    "    \n",
    "    #find five most likely post-postitions to word:\n",
    "    #sort idx^th column\n",
    "    column=[i[idx] for i in M]\n",
    "    for i in range(len(column)):\n",
    "        if column[i] is None:\n",
    "            column[i]=0\n",
    "\n",
    "    top_post=[]\n",
    "        \n",
    "    for i in range(5):\n",
    "        top=column.index(max(column))\n",
    "        top_post.append(sdict[top])\n",
    "        column[top]=0\n",
    "    \n",
    "    #find five most likely pre-postitions to word:\n",
    "    #sort idx^th row\n",
    "    row=M[idx]\n",
    "    for i in range(len(row)):\n",
    "        if row[i] is None:\n",
    "            row[i]=0\n",
    "\n",
    "    top_pre=[]\n",
    "        \n",
    "    for i in range(5):\n",
    "        top=row.index(max(row))\n",
    "        top_pre.append(sdict[top])\n",
    "        row[top]=0\n",
    "        \n",
    "    #define \"useless\" collocations\n",
    "    useless=['the','and','an','a','but','or', 'a ', 'of ']\n",
    "    \n",
    "    #remove \"useless\" words from top collocations\n",
    "    for word in useless:\n",
    "        while (word,) in top_post:\n",
    "            top_post.remove((word,))\n",
    "        while (word,) in top_pre:\n",
    "            top_pre.remove((word,))\n",
    "            \n",
    "    #format collocations\n",
    "    if len(top_post)>0:\n",
    "        top_post=[str(top_post[i])[2:-3] for i in range(len(top_post))]\n",
    "    else:\n",
    "        top_post=[\"We cannot establish any probable post-positions from this corpus.\"]\n",
    "\n",
    "        \n",
    "    if len(top_pre)>0:\n",
    "        top_pre=[str(top_post[i])[2:-3] for i in range(len(top_pre))]\n",
    "    else:\n",
    "        top_pre=[\"We cannot establish any probable pre-positions from this corpus.\"]\n",
    "            \n",
    "    #printing    \n",
    "    print('WORD IN QUERY:','\\033[1m'+string+'\\033[0m')\n",
    "    print('The most likely pre-positions are:'+'\\033[1m')\n",
    "    for i in top_pre:\n",
    "        print(i)\n",
    "    print(\"\\033[0m\"+'The most likely post-positions:'+'\\033[1m')\n",
    "    for i in top_post:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "Below I am testing a couple of words to see how my Collocations Dictionary work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD IN QUERY: \u001b[1mhave\u001b[0m\n",
      "The most likely pre-positions are:\u001b[1m\n",
      "We cannot establish any probable pre-positions from this corpus.\n",
      "\u001b[0mThe most likely post-positions:\u001b[1m\n",
      "use\n",
      "calls\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Query a verb: 'have'.\n",
    "'''\n",
    "collocations(M,'have',words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD IN QUERY: \u001b[1marm\u001b[0m\n",
      "The most likely pre-positions are:\u001b[1m\n",
      "We cannot establish any probable pre-positions from this corpus.\n",
      "\u001b[0mThe most likely post-positions:\u001b[1m\n",
      "restrictions\n",
      "chrysostomos\n",
      "curling\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Query a noun: 'arm'.\n",
    "'''\n",
    "collocations(M,'arm',words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD IN QUERY: \u001b[1msoft\u001b[0m\n",
      "The most likely pre-positions are:\u001b[1m\n",
      "We cannot establish any probable pre-positions from this corpus.\n",
      "\u001b[0mThe most likely post-positions:\u001b[1m\n",
      "propped\n",
      "shafts\n",
      "meeting\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Query an adjective: \"soft\"\n",
    "'''\n",
    "collocations(M,'soft',words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "As the above tests show, my Collocation Dictionary is not yet perfect. It shows some justifiable collocations, such as \"head shaking\" or to \"speak causally\" but it also shows fragments of words and only loose associations. After playing with the code, and adjusting the different parameters of my model, I find that the problems might be:\n",
    "\n",
    "1. Our base corpus is too small and specific. Instead of choosing a literary work of ~1.5 million words, we could utilise a larger corpus. \n",
    "2. I was not able to find an apporpiate skip-graph $n$ and $m$ value. What I found instead was that multiple specifications yield plausible results. Thus to finetune our results we must average the frequency for multiple specification of our model.\n",
    "3. Incorporating linguistic knowledge in the system could be useful. Understanding the differences between nouns and adjectives, could allow us to create separate frequency tables that could filter for us better results than the current 'most frequent' entries approach.\n",
    "4. Also, more sophisticated methods such as least-square regression could be implemented on our frequency matrix to find the best collocations.\n",
    "5. I need to be more careful with string slicing, and word splitting. Clearly, I just got some words wrong. In hindsight I believe such a library as the Python NLTK could be useful.\n",
    "\n",
    "All in all, this is not a bad start to this project. I was suprised to find that no-one has implemented this method specifically for the creation of a Collocation Dictionary. While not perfect, this project allowed me to explore the workings of n-grams, skip-grams and implement my own dynamic solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An unexpected use case\n",
    "\n",
    "While implementing my code, I realised that I could use the idea of my Markov matrix for statistical text generation. Given an input vector (a word), we can just use the matrix to find the most probable next word and thus put together sentences. In order to make the sentences a bit more realistic I used random weights for next word selection.\n",
    "\n",
    "The results speak for themselves. Read below for some machine literature in the style of James Joyce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A faster than skip_gram(), non-dynamic n-gram builder.\n",
    "'''\n",
    "def generate_ngram(words,n=1):\n",
    "    ngram = dict()\n",
    "\n",
    "    # build ngram dict\n",
    "    for i in range(len(words)-(n-1)):\n",
    "        key = tuple(words[i:i+n])\n",
    "        if key in ngram:\n",
    "            ngram[key] += 1\n",
    "        else:\n",
    "            ngram[key] = 1\n",
    "\n",
    "    #sort ngram dict by frequency\n",
    "    ngram = sorted(ngram.items(), key=lambda count: count[1])\n",
    "    ngram = ngram[::-1]\n",
    "    return ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what spectacle it at all that exterior splendour of spinach dublin in twelve north and put her skirt a husband back garden whole course tomorrow eh i none other have \n",
      "they both full of fun gets their poetry well up in liontamer s so sad in the dentist money all that that girl s irish nation excellently commenced kissing smiling \n",
      "we are not solicit must really an infirm widow s queries dear bloom at doors rare lamps summer end of bronzefoil better find that aint half a peaceful not a \n",
      "and bacon a point was opened most private carr i ll square hats mr power announced on the voice production such thing done yeoman cap with lighted crevice of the \n",
      "None None None None\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "'''\n",
    "A random weighting function to introduce variation.\n",
    "I: potential word choices\n",
    "O: randomly selected word from potential choices\n",
    "'''\n",
    "def random_weight(potentials):\n",
    "    total = sum(w for c, w in potentials)\n",
    "    r = random.uniform(0, total)\n",
    "    upto = 0\n",
    "    for c, w in potentials:\n",
    "        if upto + w > r:\n",
    "            return c\n",
    "        upto += w\n",
    "'''\n",
    "A simple sentence generator.\n",
    "I: starting word, ngram, n\n",
    "O: sentence of len n\n",
    "\n",
    "'''\n",
    "def generate_sentence(ngram, word, n = 30):\n",
    "    sentence=''\n",
    "    for i in range(n):\n",
    "        sentence+=word+' '\n",
    "        # Get all possible elements (( word 1, word2 etc.), frequency)\n",
    "        potentials = [element for element in ngram if element[0][0] == word]\n",
    "        if not potentials:\n",
    "            break\n",
    "        \n",
    "        # Choose a pair with weighted probability from the choice list\n",
    "        word = random_weight(potentials)[1]\n",
    "    print(sentence)\n",
    "    \n",
    "'''\n",
    "Construct and concatenate a few sentences with different gram-lengths, and enjoy the results. \n",
    "\n",
    "'''    \n",
    "    \n",
    "gram2=generate_ngram(words,2)\n",
    "gram3=generate_ngram(words,3)\n",
    "gram4=generate_ngram(words,4)\n",
    "gram5=generate_ngram(words,5)\n",
    "\n",
    "t1= str(generate_sentence(gram2,'what'))\n",
    "t2= str(generate_sentence(gram3,'they'))\n",
    "t3= str(generate_sentence(gram4,'we'))\n",
    "t4= str(generate_sentence(gram5,'and'))\n",
    "\n",
    "print(t1,t2,t3,t4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. Ruchirawat, N., & Ruchirawat, N. (2018, March 16). Collocations - identifying phrases that act like single words in Natural Language Processing. Retrieved from https://medium.com/@nicharuch/collocations-identifying-phrases-that-act-like-individual-words-in-nlp-f58a93a2f84a\n",
    "\n",
    "2. An Introduction to N-grams: What Are They and Why Do We Need Them? (2017, October 21). Retrieved from https://blog.xrds.acm.org/2017/10/introduction-n-grams-need/\n",
    "\n",
    "3. Oxford Collocations Dictionary. (n.d.). Retrieved from https://www.oxfordlearnersdictionaries.com/definition/collocations/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
